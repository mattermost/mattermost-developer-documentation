---
title: "From OpenVPN to Pritunl VPN: The transition"
slug: pritunl
date: 2020-05-04T12:00:00-04:00
author: Angelos Kyratzakos
github: angeloskyratzakos
community: angelos.kyratzakos
---

// JW note: I think a small section here or in the below paragraph talking about the importance of a good and reliable VPN when running a SaaS service while working remotely would be good. Gives people who aren't familiar with VPNs some background on why this was important for us to do

Many of us we are using VPN to access internal-private resources of the infrastructure. A majority of which is using OpenVPN as a solution to host those VPN connections on their own servers. [OpenVPN](https://en.wikipedia.org/wiki/OpenVPN) is a widely used software which was also our selection to be used as a quickly and reliable solution to access our internal infrastructure. After using it a while we needed a better solution in terms of:
- High Availability (HA)
- Auditing
- Supporting better access control (e.g. only SREs can access production servers and developers to access development account)
- Working with SSO, particularly OneLogin
After some investigation of the team, we ended up with [Pritunl](https://pritunl.com/) as it covered our needs and its pricing was reasonably for our scale.

// JW note: Maybe a small section here talking about Pritunl a little more. e.g. a bit about the company, the product, why we liked it, etc.

After some investigation of the team, we ended up with [Pritunl](https://pritunl.com/) as it covered our needs and its pricing was reasonably for our scale.

#### Pritunl - Infrastructure
To deploy Pritunl in our infrastructure, we used [Terraform](https://www.terraform.io/). The module we wrote can be found [here](https://github.com/mattermost/mattermost-cloud-monitoring/tree/master/terraform/aws/modules/pritunl) as an example for how to deploy Pritunl. The infrastructure consists of:
- 1 Route53 record,  
- 1 Network Load Balancer, 
- 2 AutoScaling Groups (ASG) and 
- a MongoDB Atlas.

{{< figure src="/blog/2020-05-04-pritunl/Pritunl_architecture.png" alt="Pritunl Architecture">}}

##### Why 2 AutoScaling Groups?
The selection of 2 ASGs with one instance has been done due to the nessesity of having the same Elastic Network Interface (ENI), which results on having the same private and public IPs. This is useful on whitelisting those IPs into the Security Groups that Pritunl-VPN needs to access internally such as our internal Gitlab. This can be achieved by attaching those ENIs as a secondary network interfaces on the instance, check appendix [Attach second ENI](#attach-second-eni)

You can create manually 2 ENIs (we selected the 6th address of each subnet 10.0.0.6 and 10.0.16.6) and attach Public IPs to them. Then you can provide the list of the ENI IDs with the variable (list) `fixed_eni` on terraform.

##### MongoDB Atlas
Initially, we checked the [AWS DocumentDB](https://aws.amazon.com/documentdb) solution, but it was quite expensive, as it starts from $0.28/hr for 1 db.r5.large instance. 
Next, we deployed 2 instances and installed/configured a MongoDB cluster, but maintaining and making sure that HA works was a big overhead. 

Thus, we selected to use [MongoDB Atlas](https://www.mongodb.com/cloud/atlas) as it is cheaper(starts also from free tier) and easier to setup.
After the creation of the MongoDB Atlas, we added Pritunl's Public IPs on the whitelist of the Atlas cluster and we connected to the Atlas cluster locally to create a new Database in it with a name `pritunl` ([guide on how-to-connect](https://docs.atlas.mongodb.com/getting-started/)). Also check [Pritunl documentation for MongoDB Atlas.](https://docs.pritunl.com/docs/mongodb-atlas)

To setup the MongoDB URI which looks like: `mongodb+srv://pritunl:password@pritunl-mongodb-xxxxx.mongodb.net/pritunl` use the variable (string) `mongodb_uri` on terraform.

##### Network Load Balancer
The Network Load Balancer (NLB) which is in front of the instances which has 5 listeners as shown below. There are 3 listeners for VPN (on ports 1194, 1195, 1196) that can be used for the servers inside the Pritunl.

{{< figure src="/blog/2020-05-04-pritunl/Pritunl_NetworkLB.png" alt="Pritunl Network Load Balancer">}}


#### Pritunl - Configuration
Pritunl is installed via the [userdata](https://github.com/mattermost/mattermost-cloud-monitoring/tree/master/terraform/aws/modules/pritunl/userdata.sh). As the instances do not store any configuration items except the MongoDB URI, each instance needs toconnect to the MongoDB as it gets all the configuration is needed and then the instance joins the Pritunl cluster.


##### Initial setup
As per [Pritunl documentation](https://docs.pritunl.com/docs/configuration-5#initial-setup):
- SSH into one of the pritunl instances by using [Session-manager](https://console.aws.amazon.com/systems-manager/session-manager/sessions?region=us-east-1),
- run: `sudo pritunl default-password` to get the default username and password,
- go to your login page of Pritunl and use the credentials from previous step,
- create a user `pritunl` and add a new password.

##### Organization and server setup
As per [Pritunl documentation](https://docs.pritunl.com/docs/connecting):
- go into *Users* section and create a new organization, i.e. `devs_org`
- go into *Servers* section and create a new server,
  - Name: `devs_server`
  - Port: `1194` or any other port that you have setup for VPN access, check [NLB](#network-load-balancer)
  - DNS Server: `10.0.0.2, 8.8.8.8` where 10.0.0.2 is the DNS resolution of the subnet (2nd IP of each subnet is used for DNS resolution and our VPC where Pritunl is running has peering with all the other VPCs, so it is able to resolve all the names inside our network)
  - save it
- remove the `0.0.0.0/0` route from the `devs_server`
- select the `Attach Organization` to attach `devs_org` with `devs_server`
- select the `Attach host` to attach the 2 hosts(instances) into it
- need to add all the routes that this server will have access(other VPCs)
  - select `Add Route` (make sure at the bottom the server points to `devs_server`)
    - routes to add and each comment for clarity i.e:
    - 10.8.0.0/16 prod VPC
    - 10.16.0.0/16 staging VPC
    - 10.0.0.2/32 DNS resolution (This is needed as we have set it up above on the `DNS Server`)

- Start the server with `Start Server` button



#### Enabling OneLogin
To enable Onelogin, Pritunl Enterprise (with subscription) is needed. Otherwise the configuration is not appearing into the `Settings` on the top right.
- Single Sign-On: `OneLogin`
- OneLogin App ID: `1234567`
- SAML Sign-On URL: `https://your-company.onelogin.com/trust/saml2/http-redirect/sso/xxxx-xxxx-xxxx-xxxx`
- SAML Issuer URL: `https://app.onelogin.com/saml/metadata/xxxx-xxxx-xxxx-xxxx`
- OneLogin API Client ID and OneLogin API Client Secret and the SAML Certificate.




#### Appendix
##### Attach second ENI
Below is the bash script to attach a second ENI for Ubuntu 18.04 as per [AWS documentation](https://aws.amazon.com/premiumsupport/knowledge-center/ec2-ubuntu-secondary-network-interface/)
```bash
# -----  Add fixed Network Interface -----
printf "\n### Installing AWS CLI ###\n"
apt  install awscli -y

printf "\n### Attaching ENI to instance ###\n"
INSTANCEID=$(curl http://169.254.169.254/latest/meta-data/instance-id)
MACS=$(curl http://169.254.169.254/latest/meta-data/network/interfaces/macs/ | head -n1)
SUBNETID=$(curl "http://169.254.169.254/latest/meta-data/network/interfaces/macs/$MACS/subnet-id")
NETWORKINTERFACEID=$(aws ec2 describe-network-interfaces --filters Name=tag:OnlyFor,Values=pritunl Name=status,Values=available Name=subnet-id,Values=$SUBNETID --query 'NetworkInterfaces[0].NetworkInterfaceId' --region us-east-1 --output text)
NETWORKINTERFACEIP=$(aws ec2 describe-network-interfaces --network-interface-ids $NETWORKINTERFACEID --region us-east-1 --query 'NetworkInterfaces[].[PrivateIpAddress]' --output text)
aws ec2 attach-network-interface --network-interface-id $NETWORKINTERFACEID --instance-id $INSTANCEID --device-index 1 --region us-east-1

printf "\n### Configuring instance to use secondary ENI ###\n"
SUFFIXDEFAULTIP=$(echo $NETWORKINTERFACEIP | sed 's/\.[^.]*$//')
cat <<EOF > /etc/netplan/51-eth1.yaml
network:
  version: 2
  renderer: networkd
  ethernets:
    eth1:
      addresses:
       - $NETWORKINTERFACEIP/20
      dhcp4: no
      routes:
       - to: 0.0.0.0/0
         via: $SUFFIXDEFAULTIP.1 # Default gateway
         table: 1000
       - to: $NETWORKINTERFACEIP
         via: 0.0.0.0
         scope: link
         table: 1000
      routing-policy:
        - from: $NETWORKINTERFACEIP
          table: 1000
EOF
netplan --debug apply
```

##### Pritunl API sample
In order to invoke the [Pritunl API](https://pritunl.com/api.html), we had to do some changes on the python code as below:

```python
import requests, time, uuid, hmac, hashlib, base64
BASE_URL = 'https://localhost'
API_TOKEN = 'p7g444S3IZ5wmFvmzWmx14qACXdzQ25b'
API_SECRET = 'OpS9fjxkPI3DclkdKDDr6mqYVd0DJh4i'


def auth_request(method, path, headers=None, data=None):
    auth_timestamp = str(int(time.time()))
    auth_nonce = uuid.uuid4().hex
    auth_string = '&'.join([API_TOKEN, auth_timestamp, auth_nonce,
        method.upper(), path])
    auth_string_bytes = bytes(auth_string, 'utf-8')
    api_secret_bytes = bytes(API_SECRET, 'utf-8')

    auth_signature = base64.b64encode(hmac.new(
        api_secret_bytes, auth_string_bytes, hashlib.sha256).digest())
    auth_headers = {
        'Auth-Token': API_TOKEN,
        'Auth-Timestamp': auth_timestamp,
        'Auth-Nonce': auth_nonce,
        'Auth-Signature': auth_signature,
    }
    if headers:
        auth_headers.update(headers)
    return getattr(requests, method.lower())(
        BASE_URL + path,
        headers=auth_headers,
        data=data,
    )
```
